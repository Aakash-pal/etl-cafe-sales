<!-- Banner Image -->
<p align="center">
  <img src="SQL_Enhancement/data_architecture_cafe_sales.png" alt="ETL Project Architecture" width="700"/>
</p>

# â˜• Cafe Sales ETL Pipeline

A modular ETL project designed to clean and transform a messy cafÃ© transaction dataset using **pure SQL**, **Airflow**, and **Power BI** for analytics.  
This end-to-end pipeline simulates real-world dirty data cleanup, automation, and dashboarding â€” all with **minimal Python** and **maximum SQL logic**.

---

## ğŸš€ Overview

This project started as a CSV cleanup exercise and evolved into a structured, production-style ETL pipeline:

- Ingests messy sales data from `dirty_cafe_sales.csv`
- Performs deep cleaning using **PostgreSQL CTEs**
- Automates transformations via **Apache Airflow**
- Visualizes results through **Power BI dashboards**

Whether you're learning data engineering, building a portfolio, or testing out orchestration tools â€” this project is designed to showcase core concepts clearly.

---

## ğŸ§° Tech Stack

| Layer              | Tools Used                         |
|--------------------|------------------------------------|
| Orchestration      | Apache Airflow (LocalExecutor)     |
| Data Cleaning      | PostgreSQL (via SQL scripts)       |
| Python Integration | psycopg2, Docker                   |
| Dashboarding       | Power BI                           |
| Scheduling         | Windows Task Scheduler (Archived)  |
| Version Control    | Git, GitHub                        |

---

## ğŸ“‚ Project Structure

```plaintext
ETL_PROJECT/
â”‚
â”œâ”€â”€ cafe-sales-etl/
â”‚   â”œâ”€â”€ airflow/            â†’ Apache Airflow DAGs, docker-compose, configs
â”‚   â”œâ”€â”€ docker/             â†’ Dockerfile to containerize pipeline
â”‚   â”œâ”€â”€ raw data/           â†’ Input CSV files (dirty_cafe_sales.csv)
â”‚   â”œâ”€â”€ scheduling/         â†’ Batch files for legacy scheduling (archived)
â”‚   â”œâ”€â”€ scripts/            â†’ Main orchestrator `run_sql_etl.py`, archived `etl_cafe_sales.py`
â”‚   â”œâ”€â”€ sql_etl/            â†’ Core SQL cleaning scripts
â”‚   â””â”€â”€ power_bi/           â†’ Dashboards and reports
â”‚
â”œâ”€â”€ SQL_Enhancement/        â†’ Extended SQL logic (joins, inference)
â”‚   â”œâ”€â”€ README.md           â†’ Explanation of logic and rules
â”‚   â””â”€â”€ data_architecture_cafe_sales.png â†’ Architecture diagram
â”‚
â”œâ”€â”€ logs/                   â†’ Logs from scheduled jobs
â”œâ”€â”€ .gitignore              â†’ Excludes compiled files, logs, etc.
â””â”€â”€ requirements.txt        â†’ Python dependencies
---

## âš™ï¸ How It Works

### 1. Raw Import
- Dirty sales data is loaded into a PostgreSQL raw_cafe_sales table using Python (run_sql_etl.py).

### 2. SQL Cleaning
- SQL scripts use CTEs to infer missing values like item, quantity, price, location, and payment_method.

### 3. Staging Table
- Cleaned and inferred data is written to staging_cafe_sales.

### 4. Automation
- The entire pipeline is containerized using Docker and scheduled via Apache Airflow DAG.

### 5. Analysis
- Cleaned data is used in Power BI for interactive reporting.

---

## ğŸ“ Notable Enhancements

- âœ… Inference rules (e.g., guessing item from price and location)
- âœ… Context-aware NULL handling
- âœ… Robust SQL type casting and error filtering
- âœ… Automated DAG with logs
- âœ… Validated transformation using test transaction TXN_9999999

---
## ğŸ“ Key Files

|File/Folder |	Description |
|------------|--------------|
|scripts/run_sql_etl.py	|âœ… Main orchestrator using psycopg2|
|scripts/etl_cafe_sales.py|	ğŸ—ƒï¸ Archived version from initial setup|
|sql_etl/*.sql |	Raw and staging SQL logic|
|power_bi/dashboard.pbix|	Final report visualizing cleaned data|
|airflow/docker-compose.yml|	Airflow orchestration setup|
|scheduling/run_etl.bat	| Archived manual trigger method (Windows)|

---

## ğŸ”— Linked Sections
- ğŸ“Š SQL Enhancement Folder
- ğŸ“ˆ Power BI Visuals
- ğŸ—ƒï¸ Archived Python Script

---

Modular ETL pipeline project using SQL, Airflow, and Power BI with real-world dirty data.
## ğŸ§  What I Learned
- Writing production-style SQL with CTEs
- Orchestrating data pipelines via Airflow
- Debugging Docker and Airflow containers
- Building traceable and testable ETL flows
- Structuring a portfolio-ready GitHub project

---

## âœ… Changelog
- Week 1â€“2: CSV import, raw table, and cleaning logic
- Week 3â€“4: Inference logic via SQL + staging table
- Week 5: Power BI integration
- Week 6â€“7: Docker + Task Scheduler (archived)
- Week 8â€“9: Airflow integration
- Week 10â€“Final: Cleanup, testing, and README polish

---
## ğŸ™‹â€â™€ï¸ About Me

ğŸ‘©â€ğŸ’» **Aakash Pal**  
ğŸŒ Hyderabad, India  
ğŸ¯ Aspiring Cloud ETL Engineer  
ğŸ”— GitHub: [Aakash Pal](https://github.com/Aakash-pal)
- This project was built to gain hands-on experience in real-world data engineering practices â€” focusing on raw data challenges, SQL mastery, and visual storytelling.

---

## ğŸ“¬ Contact

If you're hiring, collaborating, or want to give feedback â€” feel free to reach out via LinkedIn or raise an issue in this repo.